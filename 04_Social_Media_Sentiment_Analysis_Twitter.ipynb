{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anson3208/Sentiment-Textmining-Analysis-Learning/blob/main/04_Social_Media_Sentiment_Analysis_Twitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 4 - Social Media Sentiment Analysis Twitter\n"
      ],
      "metadata": {
        "id": "gnue4wBGGdAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes will be adopted to perform Sentiment Analysis"
      ],
      "metadata": {
        "id": "73oJHABHARGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Large Movie Review Dataset"
      ],
      "metadata": {
        "id": "iu0A5GzWYSKa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Large Movie Review Dataset is a corpus of 50,000 movie reviews from IMDB that have been classified as either positive or negative. More information about the dataset can be found at https://ai.stanford.edu/~amaas/data/sentiment/. "
      ],
      "metadata": {
        "id": "aYD73eRqblOY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code downloads a copy of the Large Movie Review Dataset and saves it in a variable `imdb_corpus`. "
      ],
      "metadata": {
        "id": "RkwZzYJrf4Mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request, json \n",
        "imdb_corpus = []\n",
        "with urllib.request.urlopen(\"https://storage.googleapis.com/wd13/IMDBReviewSent.txt\") as url:\n",
        "  for line in url.readlines():\n",
        "    imdb_corpus.append(line.decode().split('\\t'))"
      ],
      "metadata": {
        "id": "JVwy5gnFYSVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`imdb_corpus` is a list. Each element of the list is another list which stores a document and its label."
      ],
      "metadata": {
        "id": "ejA8KxjOgO1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print the text and label of document 16\n",
        "docid = 1\n",
        "print(imdb_corpus[docid])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xppAWdKJgRD3",
        "outputId": "c178a3ca-c69d-4b43-94fa-7cecb3c23301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['positive', 'A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master\\'s of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell\\'s murals decorating every surface) are terribly well done.\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print the label of document 16\n",
        "docid = 16\n",
        "print(imdb_corpus[docid][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujCCOXJFmp6o",
        "outputId": "f410ee61-57e6-4abe-d5bb-b548233ff80e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print the text of document 16\n",
        "docid = 16\n",
        "print(imdb_corpus[docid][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heI9vtSzmqHT",
        "outputId": "ec3981da-0f02-4e5b-c00e-57c8027adc82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some films just simply should not be remade. This is one of them. In and of itself it is not a bad film. But it fails to capture the flavor and the terror of the 1963 film of the same title. Liam Neeson was excellent as he always is, and most of the cast holds up, with the exception of Owen Wilson, who just did not bring the right feel to the character of Luke. But the major fault with this version is that it strayed too far from the Shirley Jackson story in it's attempts to be grandiose and lost some of the thrill of the earlier film in a trade off for snazzier special effects. Again I will say that in and of itself it is not a bad film. But you will enjoy the friction of terror in the older version much more.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a tokenizer"
      ],
      "metadata": {
        "id": "12DyunxPvThg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a function `tokenize` that takes a string and returns a list of tokens. "
      ],
      "metadata": {
        "id": "J7L7Du1QvThr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Copying tokenizer from week 4 solution\n",
        "import re\n",
        "def tokenize(doc):\n",
        "  emoti_list = [':)','(:',':(','):',':D','D:',':P','P:',':V','V:',':/','/:',':\\\\','\\\\:',':|','|:',\n",
        "                ';)','(;',';(',');',';D','D;',';P','P;',';V','V;',';/','/;',';\\\\','\\\\;',';|','|;',\n",
        "                ':-)','(-:',':-(',')-:',':-D','D-:',':-P','P-:',':-V','V-:',':-/','/-:',':-\\\\',\n",
        "                '\\\\-:',':-|','|-:',';-)','(-;',';-(',')-;',';-D','D-;',';-P','P-;',';-V','V-;',\n",
        "                ';-/','/-;',';-\\\\','\\\\-;',';-|','|-;']\n",
        "  tokenizer_pattern = re.compile('|'.join([\n",
        "      '|'.join([re.escape(e) for e in emoti_list]),\n",
        "      \"[A-Za-z]+(?:['-_\\.][A-Za-z]+)?\",\n",
        "      '\\.\\.+'\n",
        "      ]))\n",
        "  tokens = tokenizer_pattern.findall(doc)\n",
        "  for i in range(0,len(tokens)):\n",
        "    if re.match('\\.\\.+',tokens[i]):\n",
        "      tokens[i] = '..+'\n",
        "    else:\n",
        "      tokens[i] = tokens[i].lower()\n",
        "  return(tokens)"
      ],
      "metadata": {
        "id": "KCag6LMXvThr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import log function"
      ],
      "metadata": {
        "id": "hzNSjeBLHA-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import log\n",
        "log(1)"
      ],
      "metadata": {
        "id": "BoHVDK-qHC1Z",
        "outputId": "71fc6635-7174-4d4c-c57d-2e32c963805c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a lexicon"
      ],
      "metadata": {
        "id": "dxttX-SIh1oi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate sentiment scores for every token in the corpus, using the method discussed in class. Store these scores in a dictionary called `lexicon`. "
      ],
      "metadata": {
        "id": "1D5FlYbhi5lJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "corpus_positive_count = 0\n",
        "corpus_negative_count = 0\n",
        "\n",
        "token_positive_count = {}\n",
        "token_negative_count = {}\n",
        "\n",
        "unique_tokens = set()\n",
        "lexicon = {}\n",
        "\n",
        "for doc in imdb_corpus:\n",
        "  label = doc[0]\n",
        "  tokens = tokenize(doc[1])\n",
        "\n",
        "#calculate corpus positive/negative count\n",
        "  if label == 'positive':\n",
        "    corpus_positive_count += 1\n",
        "  else:\n",
        "    corpus_negative_count +=1\n",
        "\n",
        "#calculate token positive/negative count\n",
        "  for token in set(tokens):\n",
        "    unique_tokens.add(token)\n",
        "    if label == 'positive':\n",
        "      if token not in token_positive_count:\n",
        "        token_positive_count [token] = 1\n",
        "      else:\n",
        "        token_positive_count [token] += 1\n",
        "    else:\n",
        "      if token not in token_negative_count:\n",
        "        token_negative_count [token] = 1\n",
        "      else:\n",
        "        token_negative_count [token] += 1\n",
        "\n",
        "#calculate score for lexicon\n",
        "for token in unique_tokens:\n",
        "  if token not in token_positive_count or token not in token_negative_count:\n",
        "    continue\n",
        "  else:\n",
        "    lexicon[token] = log((token_positive_count[token]/corpus_positive_count)/(token_negative_count[token]/corpus_negative_count))\n",
        "\n",
        "#Update 'not' to 0 from lexicon and adjust negation calculation in score message function\n",
        "lexicon['not'] =0\n",
        "\n"
      ],
      "metadata": {
        "id": "8zPLVa33h3IS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check lexicon\n",
        "print(lexicon['movie'])\n",
        "print(lexicon['good'])"
      ],
      "metadata": {
        "id": "GiyeXrBD_zAf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c93db8d-8100-4306-bfb0-6675bd7e96e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.1694416703576676\n",
            "-0.03189482684337694\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a score message function"
      ],
      "metadata": {
        "id": "e6rrKlK0kY77"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a funciton `score_message` that takes a message `doc` and returns a sentiment score, using the method discussed in class. "
      ],
      "metadata": {
        "id": "jn6rOE5JnPqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment_score(document):\n",
        "    \"\"\"Calculate the sentiment score for a given document\"\"\"\n",
        "    sentiment_score = log(corpus_positive_count/corpus_negative_count)\n",
        "    tokens = tokenize(document)\n",
        "\n",
        "    word_after_not = None                                   # the word after negation\n",
        "    if 'not' in tokens:                                     # check if 'not' in tokens\n",
        "        word_after_not = tokens.index('not') + 1            # position of the word after not in tokens\n",
        "\n",
        "    for index, token in enumerate(tokens):\n",
        "        if token in lexicon:\n",
        "            if index != word_after_not:\n",
        "                sentiment_score += lexicon[token]\n",
        "            else:\n",
        "                sentiment_score += lexicon[token] * -1      # Multiplied by -1 for the score adjusment of negation\n",
        "            #print(token,lexicon[token],sentiment_score)\n",
        "    #print('Total score', sentiment_score)    \n",
        "    return sentiment_score"
      ],
      "metadata": {
        "id": "ESGd98pVuF1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Sentiment Score\n",
        "print(sentiment_score('It is amazing '))\n",
        "print(sentiment_score('It is not amazing'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cyAC0ZzEuey",
        "outputId": "c074e3df-35db-41fa-9df5-0e2846a3f972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.2607217956781696\n",
            "-1.252024365760766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Live Twitter Data"
      ],
      "metadata": {
        "id": "HoQ1o-ixdCyV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To download live Twitter data, you will need a bearer token. To get a bearer token, you will need a devloper account. Go to https://developer.twitter.com/en to sign up."
      ],
      "metadata": {
        "id": "9xfZ9qPwcXh2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have your bearer token, create a new text file on your computer called `'twitter_bearer_token.txt'`. This file should contain a single line consiting of your bearer token and nothing else (no white space, no new lines, just the token). "
      ],
      "metadata": {
        "id": "QSHHDF4Hc9T0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below will prompt you to upload a file from your computer. Upload 'twitter_bearer_token.txt'. Your bearer token will be saved to a string twitter_bearer_token."
      ],
      "metadata": {
        "id": "2f0sXpiXfAPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded_files = files.upload()\n",
        "twitter_bearer_token = uploaded_files['twitter_bearer_token.txt'].decode()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "61Vh3EyTdDXg",
        "outputId": "a0010423-c1b0-4650-af0e-afd42656e075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c83caa8c-5ac1-4ade-bde2-dd76f34770ab\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c83caa8c-5ac1-4ade-bde2-dd76f34770ab\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving twitter_bearer_token.txt to twitter_bearer_token.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we define the function get_tweets. This function returns tweets matching a specificed query. It takes the following parameters:\n",
        "\n",
        "query your search query\n",
        "bearer_token your Twitter API bearer token\n",
        "number_of_tweets the number of tweets you want to return\n",
        "This code uses the Twitter API's Recent Search endpoint. See https://developer.twitter.com/en/docs/twitter-api/tweets/search/introduction for documentation. This endpoint only provides access to tweets from the past week."
      ],
      "metadata": {
        "id": "FCklwaZffK6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "def get_tweets(query,bearer_token,number_of_tweets):\n",
        "  tweets = []\n",
        "  next_token = None\n",
        "  while len(tweets)<number_of_tweets:\n",
        "    response = requests.get(\n",
        "        url = 'https://api.twitter.com/2/tweets/search/recent',\n",
        "        params = {\n",
        "          'query':query,\n",
        "          'next_token':next_token},\n",
        "        headers = {'authorization' : 'bearer '+bearer_token} \n",
        "        )\n",
        "    response_json = response.json()\n",
        "    for tweet in response_json['data']:\n",
        "      tweets.append(tweet)\n",
        "    if 'next_token' not in response_json['meta']:\n",
        "      break\n",
        "    next_token = response_json['meta']['next_token']\n",
        "  return(tweets)"
      ],
      "metadata": {
        "id": "8Ep2PzvrfPIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code downloads 500 recent tweets containing the words \"nike\" or \"adidas\" and saves them in tweets."
      ],
      "metadata": {
        "id": "HdVQG2erfREx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = get_tweets('nike OR adidas',twitter_bearer_token,500)"
      ],
      "metadata": {
        "id": "CK55Id6bfU7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each tweet is represented by a dictionary with three fields:\n",
        "\n",
        "edit_history_tweet_ids\n",
        "id\n",
        "text\n",
        "It is possible to download additional fields. See the API documentation referenced above."
      ],
      "metadata": {
        "id": "5OIWZNkufXVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets[200]"
      ],
      "metadata": {
        "id": "u-FU8v9yfY6B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12c7d2d3-02fb-4dab-8476-f4ac3a06c1b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'edit_history_tweet_ids': ['1624532227697655809'],\n",
              " 'id': '1624532227697655809',\n",
              " 'text': '@alinvenex @boraejk @kinetix_tr @PUMATurkiye @lumberjackTR @Nike @ninewestturkiye @PolarisTurkiye @adidas @DockerSbyGerli @butigocom @lottosport @Reebok @uspoloassntr @Vans Ä°NSANLARA YIPRANMIÅž YIRTIK AYAKKABILAR YOLLUYORLARMIÅž AYAKKABI MARKALARINI ETÄ°KETLEYÄ°P DESTEK Ä°STÄ°YORUZ @kinetix_tr @PUMATurkiye @lumberjackTR @Nike @ninewestturkiye @PolarisTurkiye @adidas @DockerSbyGerli @butigocom @lottosport @Reebok @uspoloassntr @Vans'}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code allows you to download the tweets into a json file that you can save to your computer."
      ],
      "metadata": {
        "id": "dMdxsu1Dfc7y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyze Twitter Data"
      ],
      "metadata": {
        "id": "ietSm--hqKSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('tweets.json','w') as f:\n",
        "  f.write(json.dumps(tweets))\n",
        "files.download('tweets.json')"
      ],
      "metadata": {
        "id": "GXAmP_Dcfd1i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "cd1ed93f-752a-484a-948e-3c4a5b2909a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e378f6c6-3a49-4950-9220-a09c6d477722\", \"tweets.json\", 139491)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose a topic and perform some analysis of tweets related to that topic. Use some (or all) of the statistics we discussed in class:\n",
        "* Number of Tweets\n",
        "* Share of Voice\n",
        "* Positive Percent\n",
        "* Negative Percent\n",
        "* Net Positive Percent"
      ],
      "metadata": {
        "id": "e-tCh_U8qMg8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic: Explore Nike and Adidas on Twitter"
      ],
      "metadata": {
        "id": "1DzhWWbdf7jT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Subset text in tweet for analysis\n",
        "tweets_text = [tweet['text'] for tweet in tweets]\n",
        "\n",
        "#test\n",
        "tweets_text[0]"
      ],
      "metadata": {
        "id": "a4iasdfkiOZ6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "e427e793-bd0d-4368-e07c-d30a51bbc4b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ðŸ“ŒAdidas Live @Gman10312619 @Alphie40750663 @chicktheboy @Thorste49651905 @hk_azuki @sipcocacola @dika48470938 @TheAntzNest @aleksandrovn696 @nic_pic10 @MoCryptoFTW @portia15805644 https://t.co/uvyU9oqweG'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we count how many tweets are related to Nike and Adidas, and how many of those are positive and negative."
      ],
      "metadata": {
        "id": "kNmdhvUigUz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Count how many tweets for Nike and Adidas and sentiment scores\n",
        "\n",
        "nike_total_count = 0\n",
        "nike_positive_count = 0\n",
        "nike_negative_count = 0\n",
        "adidas_total_count = 0\n",
        "adidas_positive_count = 0\n",
        "adidas_negative_count = 0\n",
        "\n",
        "for tweet in tweets_text:\n",
        "  tweet_sentiment_score = sentiment_score(tweet)      #calculate sentiment score for each iteration\n",
        "  tweet_tokens = set(tokenize(tweet))                 #tokenize tweet\n",
        "  if 'nike' in tweet_tokens:                          #calculate for nike total_count\n",
        "    nike_total_count +=1\n",
        "    if tweet_sentiment_score >=0:                     #calculate nike positive count    \n",
        "      nike_positive_count +=1                           \n",
        "    else:\n",
        "      nike_negative_count +=1                         #calculate nike negative count\n",
        "  elif 'adidas' in tweet_tokens:\n",
        "    adidas_total_count +=1\n",
        "    if tweet_sentiment_score >=0:\n",
        "      adidas_positive_count +=1\n",
        "    else:\n",
        "      adidas_negative_count +=1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ICvaTM6BgHdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Number of Tweets for Nike and Adidas\n",
        "print('nike_total_count:', nike_total_count)\n",
        "print('nike_positive_count:',nike_positive_count)\n",
        "print('nike_negative_count:',nike_negative_count)\n",
        "print('adidas_total_count:',adidas_total_count)\n",
        "print('adidas_positive_count:',adidas_positive_count)\n",
        "print('adidas_negative_count:',adidas_negative_count)"
      ],
      "metadata": {
        "id": "Jqy4zTZcohlb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ffd5467-83bd-4e44-cd5d-9b1798848fb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nike_total_count: 188\n",
            "nike_positive_count: 121\n",
            "nike_negative_count: 67\n",
            "adidas_total_count: 269\n",
            "adidas_positive_count: 192\n",
            "adidas_negative_count: 77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then calculate the following summary statistics:\n",
        "* Number of Tweets\n",
        "* Share of Voice\n",
        "* Positive Percent\n",
        "* Negative Percent\n",
        "* Net Positive Percent"
      ],
      "metadata": {
        "id": "g6-3NOqZrkQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nike_corpus_summary = {\n",
        "    'NTweets' : nike_total_count,\n",
        "    'ShareofVoice' : 100*nike_total_count/(nike_total_count+adidas_total_count),\n",
        "    'PositivePct' : 100*nike_positive_count/nike_total_count,\n",
        "    'NegativePct' : 100*nike_negative_count/nike_total_count,\n",
        "    'NetPositivePct' : 100*(nike_positive_count-nike_negative_count)/nike_total_count\n",
        "}\n",
        "adidas_corpus_summary = {\n",
        "    'NTweets' : adidas_total_count,\n",
        "    'ShareofVoice' : 100*adidas_total_count/(nike_total_count+adidas_total_count),\n",
        "    'PositivePct' : 100*adidas_positive_count/adidas_total_count,\n",
        "    'NegativePct' : 100*adidas_negative_count/adidas_total_count,\n",
        "    'NetPositivePct' : 100*(adidas_positive_count-adidas_negative_count)/adidas_total_count\n",
        "}"
      ],
      "metadata": {
        "id": "lskvl-mCom6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We display these summary statistics in a table. "
      ],
      "metadata": {
        "id": "dHTbgDaYrqlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\")\n",
        "print(' | '+\n",
        "      'Brand          | '+\n",
        "      '# Tweets       | '+\n",
        "      'Share of Voice | '+\n",
        "      'Positive %     | '+\n",
        "      'Negative %     | '+\n",
        "      'Net Positive % | ')\n",
        "print(\"\")\n",
        "print((\" | \"+\n",
        "      \"Adidas         | \"+\n",
        "      \"{NTweets:5.0f}          | \"+\n",
        "      \"{ShareofVoice:5.2f}%         | \"+\n",
        "      \"{PositivePct:2.2f}%         | \"+\n",
        "      \"{NegativePct:2.2f}%         | \"+\n",
        "      \"{NetPositivePct:2.2f}%         | \").format(**adidas_corpus_summary))\n",
        "print((\" | \"+\n",
        "      \"Nike           | \"+\n",
        "      \"{NTweets:5.0f}          | \"+\n",
        "      \"{ShareofVoice:5.2f}%         | \"+\n",
        "      \"{PositivePct:2.2f}%         | \"+\n",
        "      \"{NegativePct:2.2f}%         | \"+\n",
        "      \"{NetPositivePct:2.2f}%         | \").format(**nike_corpus_summary))\n",
        "print(\"\")"
      ],
      "metadata": {
        "id": "Zk9Kmo4Crq8X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90a8a5a4-c2ec-4402-c120-774a94810be5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " | Brand          | # Tweets       | Share of Voice | Positive %     | Negative %     | Net Positive % | \n",
            "\n",
            " | Adidas         |   269          | 58.86%         | 71.38%         | 28.62%         | 42.75%         | \n",
            " | Nike           |   188          | 41.14%         | 64.36%         | 35.64%         | 28.72%         | \n",
            "\n"
          ]
        }
      ]
    }
  ]
}